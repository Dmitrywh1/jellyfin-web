stages:
  - init
  - tests
  - sast
  - build
  - deploy
  - dast

default:
  image: node:20-alpine
  tags:
    - docker

variables:
  IMAGE: "${CI_REGISTRY}/${CI_REGISTRY_ID}/${CI_PROJECT_NAME}"
  DOCKER_CONTAINER: $CI_PROJECT_NAME-docker
  COMPOSE_CONTAINER: $CI_PROJECT_NAME-compose
  TAG: $CI_COMMIT_SHA
  ENVIRONMENT: dev
  DEPLOY_PORTS_COMPOSE: 8080:80
  DEPLOY_PORTS_DOCKER: 8081:80
  DOCKER_COMPOSE: |
    version: '3'
    services:
      $CI_PROJECT_NAME:
        image: $IMAGE:$TAG
        container_name: $COMPOSE_CONTAINER
        ports:
          - $DEPLOY_PORTS_COMPOSE

.node_modules_cache:
  cache:
    key:
      files:
        - package.json
        - package.json
    paths:
      - node_modules

install .node_modules:
  stage: init
  extends: .node_modules_cache
  script:
    - npm install

lint:
  stage: tests
  extends: .node_modules_cache
  script:
    - npm run lint

vitest:
  stage: tests
  extends: .node_modules_cache
  script:
    - npm run test

eslint:
  stage: sast
  extends: .node_modules_cache
  script:
    - npx eslint . -f json > eslint-report.json | tee eslint-report.json
  artifacts:
    paths:
      - eslint-report.json
  allow_failure: true

audit:
  stage: sast
  extends: .node_modules_cache
  script:
    - npm audit --json > audit-report.json | tee audit-report.json
  artifacts:
    paths:
      - audit-report.json
  allow_failure: true

pre build:
  stage: build
  extends: .node_modules_cache
  script:
    - npm run build:production
  artifacts:
    paths:
      - dist/
      - Dockerfile

build:
  stage: build
  image:
    name: gcr.io/kaniko-project/executor:v1.23.2-debug
    entrypoint: ['']
  before_script:
      - echo "{\"auths\":{\"$CI_REGISTRY\":{\"username\":\"$CI_REGISTRY_USER\",\"password\":\"$CI_REGISTRY_PASSWORD\"}}}" > /kaniko/.docker/config.json
  script:
    - /kaniko/executor
      --context "${CI_PROJECT_DIR}"
      --dockerfile "./Dockerfile"
      --destination "${IMAGE}:$TAG"
      --cache=true
      --no-push-cache
      --cache-dir=/cache
  cache:
    paths:
      - /cache
  dependencies:
    - pre build
  needs:
    - pre build

.prepare_docker_work:
  image: docker:latest
  before_script:
    - mkdir ~/.ssh
    - printf "%sn" "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
    - chmod 600 ~/.ssh/id_rsa
    - ssh-keyscan -H $REMOTE_HOST_DEV >> ~/.ssh/known_hosts
    - |
      cat <<EOF > ~/.ssh/config
      Host dev
      HostName $REMOTE_HOST_DEV
      User ubuntu
      IdentityFile ~/.ssh/id_rsa
      EOF
    - docker context create $ENVIRONMENT --docker "host=ssh://ubuntu@dev"
    - unset DOCKER_HOST
    - echo -n "$CI_REGISTRY_PASSWORD" | docker login --username $CI_REGISTRY_USER --password-stdin $CI_REGISTRY

.prepare_helm_work:
  image:
    name: alpine/helm
    entrypoint: [""]
  script:
    - export KUBECONFIG=$KUBECONFIG

deploy_docker:
  stage: deploy
  extends: .prepare_docker_work
  script:
    - docker context use dev
    - docker pull $IMAGE:$TAG
    - docker stop $DOCKER_CONTAINER || true
    - docker rm $DOCKER_CONTAINER || true
    - docker run -d --name $DOCKER_CONTAINER -p $DEPLOY_PORTS_DOCKER $IMAGE:$TAG
  needs:
    - build

deploy_docker_compose:
  stage: deploy
  extends: .prepare_docker_work
  script:
    - echo -e "$DOCKER_COMPOSE" > docker-compose.yml
    - docker --context $ENVIRONMENT compose pull
    - echo $TAG
    - docker --context $ENVIRONMENT compose up -d --force-recreate --remove-orphans
  needs:
    - build

deploy_helm:
  stage: deploy
  extends: .prepare_helm_work
  script:
    - helm upgrade $CI_PROJECT_NAME charts/$CI_PROJECT_NAME/
      --debug
      --install
      --atomic
      --values charts/$CI_PROJECT_NAME/dev-values.yml
      --namespace $ENVIRONMENT
  needs:
    - build

dast_job:
  stage: dast
  image: zaproxy/zap-stable
  script:
    - zap-baseline.py -t http://$REMOTE_HOST_DEV:8080 -I
  allow_failure: true
